
kiểm tra xem có sai ở đâu không mà p_value không uniform (đây là code Selective Inference cho OracleLasso, phiên bản khác của Lasso), code lại đầy đủ các file, nếu lý thuyết chứng minh có sai thì hãy đề xuất sửa lý thuyết đó:


1. Đây là LATEX lý thuyết:
\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}

\title{Selective Inference on Oracle Trans Lasso}

\begin{document}

\maketitle

\section{Algorithm}

\noindent \textbf{Input}: Primary data \((\mathbf{X}^{0}, \mathbf{y}^{0})\) and auxiliary samples \((\mathbf{X}^{\mathcal{A}}, \mathbf{y}^{\mathcal{A}})\)

\noindent \textbf{Output}: \(\hat{\boldsymbol{\beta}}\)

\begin{itemize}
    \item Compute:
    \[
    \hat{\boldsymbol{w}} = \arg \min_{\boldsymbol{w}} \left\{ \frac{1}{2} \| \mathbf{y}^{\mathcal{A}} - \mathbf{X}^{\mathcal{A}} \boldsymbol{w} \|_2^2 + \lambda_{\boldsymbol{w}} \| \boldsymbol{w} \|_1 \right\}
    \]
    \item Let:
    \[
    \hat{\boldsymbol{\beta}} = \hat{\boldsymbol{w}} + \hat{\boldsymbol{\delta}}, \quad \hat{\boldsymbol{\delta}} = \arg \min_{\boldsymbol{\delta}} \left\{ \frac{1}{2} \| \mathbf{y}^{0} - \mathbf{X}^{0} (\hat{\boldsymbol{w}} + \boldsymbol{\delta}) \|_2^2 + \lambda_{\boldsymbol{\delta}} \| \boldsymbol{\delta} \|_1 \right\}
    \]
\end{itemize}

\section{Setup Problem}
\[
\mathbf{y}^0 \sim \mathcal{N}(\boldsymbol{\mu}^0, \Sigma^0), \quad
\mathbf{y}^\mathcal{A} \sim \mathcal{N}(\boldsymbol{\mu}^\mathcal{A}, \Sigma^\mathcal{A})
\]
\[
\mathbf{y} = \begin{pmatrix}
\mathbf{y}^0 \\ \mathbf{y}^\mathcal{A}
\end{pmatrix}, \quad
\Sigma = \begin{pmatrix}
\Sigma^0 & \mathbf{0} \\
\mathbf{0} & \Sigma^\mathcal{A}
\end{pmatrix}
\]

\section{Definitions of \(\mathbf{P}\) and \(\mathbf{Q}\)}

\begin{itemize}
    \item Projection matrix \(\mathbf{P}\): Projects \(\mathbf{y}\) to the primary data \(\mathbf{y}^0\):
    \[
    \mathbf{P} = (\mathbf{I}_{n_0}, \mathbf{0}) \in \mathbb{R}^{n_0 \times n}
    \]
    \item Projection matrix \(\mathbf{Q}\): Projects \(\mathbf{y}\) to the auxiliary data \(\mathbf{y}^\mathcal{A}\):
    \[
    \mathbf{Q} = (\mathbf{0}, \mathbf{I}_{n_\mathcal{A}}) \in \mathbb{R}^{n_\mathcal{A} \times n}
    \]
\end{itemize}

\section{Definitions of \(\boldsymbol{\eta}_j\)}

\begin{itemize}
    \item \(\boldsymbol{\eta}_j\) is the test statistic direction defined as:
    \[
    \boldsymbol{\eta}_j = \mathcal{X} \mathbf{e}_j
    \]
    where:
    \[
    \mathcal{X} = \begin{pmatrix}
    \mathbf{X}^0_{\mathcal{M}_{obs}}(\mathbf{X}^0_{\mathcal{M}_{obs}}^\top \mathbf{X}^0_{\mathcal{M}_{obs}})^{-1} \\ 
    \mathbf{0}_{n_\mathcal{A} \times |\mathcal{M}_{obs}|}
    \end{pmatrix}, \quad \mathbf{e}_j \in \mathbb{R}^{|\mathcal{M}_{obs}|}
    \]
\end{itemize}

\section{Definitions of \(\psi\), \(\gamma\), \(\nu\), and \(\kappa\)}

\begin{itemize}
    \item \(\boldsymbol{\psi}_{\mathcal{W}_z}(z)\):
    \[
    \boldsymbol{\psi}_{\mathcal{W}_z}(z) = \left((\mathbf{X}_{\mathcal{W}_z}^\mathcal{A})^\top \mathbf{X}_{\mathcal{W}_z}^\mathcal{A}\right)^{-1} (\mathbf{X}_{\mathcal{W}_z}^\mathcal{A})^\top \mathbf{Q} \mathbf{b}
    \]

    \item \(\boldsymbol{\gamma}_{\mathcal{W}_z^c}(z)\):
    \[
    \boldsymbol{\gamma}_{\mathcal{W}_z^c}(z) = (\mathbf{X}_{\mathcal{W}_z^c}^\mathcal{A})^\top \mathbf{Q} \mathbf{b} - (\mathbf{X}_{\mathcal{W}_z^c}^\mathcal{A})^\top \mathbf{X}_{\mathcal{W}_z}^\mathcal{A} \boldsymbol{\psi}_{\mathcal{W}_z}(z)
    \]

    \item \(\boldsymbol{\nu}_{\Delta_z}(z)\):
    \[
    \boldsymbol{\nu}_{\Delta_z}(z) = \left((\mathbf{X}_{\Delta_z}^0)^\top \mathbf{X}_{\Delta_z}^0 \right)^{-1} (\mathbf{X}_{\Delta_z}^0)^\top \left(\mathbf{P}\mathbf{b} - \mathbf{X}_{\mathcal{W}_z}^0 \boldsymbol{\psi}_{\mathcal{W}_z}(z)\right)
    \]

    \item \(\boldsymbol{\kappa}_{\Delta_z^c}(z)\):
    \[
    \boldsymbol{\kappa}_{\Delta_z^c}(z) = (\mathbf{X}_{\Delta_z^c}^0)^\top \left(\mathbf{P}\mathbf{b} - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_z}(z)\right) - (\mathbf{X}_{\Delta_z^c}^0)^\top \mathbf{X}_{\Delta_z}^0 \boldsymbol{\nu}_{\Delta_z}(z)
    \]
\end{itemize}

\section{Transition Points}
The transition point \(t_z\) is given by:
\[
t_z = \min \{ t_z^1, t_z^2, t_z^3, t_z^4 \}
\]
where:
\[
t_z^1 = \min_{j \in \mathcal{W}_z} \left( -\frac{\hat{w}_j(z)}{\psi_j(z)} \right)_{++}, \quad
t_z^2 = \min_{j \in \mathcal{W}_z^c} \left( \lambda_{\boldsymbol{w}}\frac{\text{sign}(\gamma_j(z)) - s_j(z)}{\gamma_j(z)} \right)_{++}
\]
\[
t_z^3 = \min_{j \in \Delta_z} \left( -\frac{\hat{\delta}_j(z)}{\nu_j(z)} \right)_{++}, \quad
t_z^4 = \min_{j \in \Delta_z^c} \left( \lambda_{\boldsymbol{\delta}} \frac{\text{sign}(\kappa_j(z)) - s'_j(z)}{\kappa_j(z)} \right)_{++}
\]
Here, \((m)_{++} = m\) if \(m > 0\), and \(+\infty\) otherwise.

\end{document}

2. Đây là LATEX lý thuyết full có chứng minh:
\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]
{geometry}
\title{Selective Inference on Oracle Trans Lasso}
% \author{Khai Tam, Huyen My}
\date{November 2024}

\begin{document}

\maketitle
\section{Algorithm}

\noindent \textbf{Input}: Primary data \((\mathbf{X}^{0}, \mathbf{y}^{0})\) and informative auxiliary samples \((\mathbf{X}^{\mathcal{A}}, \mathbf{y}^{\mathcal{A}})\)

\noindent \textbf{Output}: \(\hat{\boldsymbol{\beta}}\)

\begin{itemize}
    \item \textbf{Step 1}. Compute
    \[
    \hat{\boldsymbol{w}} = \arg \min_{\boldsymbol{w} \in \mathbb{R}^p} \left\{ \frac{1}{2} \| \mathbf{y}^{\mathcal{A}} - \mathbf{X}^{\mathcal{A}} \boldsymbol{w} \|_2^2 + \lambda_{\boldsymbol{w}} \| \boldsymbol{w} \|_1 \right\}
    \]
    where \(\lambda_{\boldsymbol{w}} = c_1 \sqrt{n_\mathcal{A}\log p}\) with some constant \(c_1\).
    
    \item \textbf{Step 2}. Let
    \[
    \hat{\boldsymbol{\beta}} = \hat{\boldsymbol{w}} + \hat{\boldsymbol{\delta}},
    \]
    where
    \[
    \hat{\boldsymbol{\delta}} = \arg \min_{{\boldsymbol{\delta}} \in \mathbb{R}^p} \left\{ \frac{1}{2} \| \mathbf{y^{0}} - \mathbf{X^{0}} (\hat{\boldsymbol{w}} + {\boldsymbol{\delta}}) \|_2^2 + \lambda_{\boldsymbol{\delta}} \| {\boldsymbol{\delta}} \|_1 \right\}
    \]
    \[
    = \arg \min_{{\boldsymbol{\delta}} \in \mathbb{R}^p} \left\{ \frac{1}{2} \| (\mathbf{y^{0}} - \mathbf{X^{0}} \hat{\boldsymbol{w}}) - \mathbf{X^{0}} {\boldsymbol{\delta}} \|_2^2 + \lambda_{\boldsymbol{\delta}} \| {\boldsymbol{\delta}} \|_1 \right\}
    \]
    and \(\lambda_{\boldsymbol{\delta}} = c_2 \sqrt{n_0\log p}\) with some constant \(c_2\).
\end{itemize}

\section{Setup Problem}
\[
\mathbf{y}^0 = (y^0_1, y^0_2, \dots, y^0_{n_0}) \sim \mathcal{N}(\boldsymbol{\mu}^0, \Sigma^0)
\]
\[
\mathbf{y}^\mathcal{A} = (y^\mathcal{A}_1, y^\mathcal{A}_2, \dots, y^\mathcal{A}_{n_\mathcal{A}}) \sim \mathcal{N}(\boldsymbol{\mu}^\mathcal{A}, \Sigma^\mathcal{A})
\]

\[ \mathbf{y} = 
\begin{pmatrix}
\mathbf{y}^0 \\ \mathbf{y}^\mathcal{A} 
\end{pmatrix},
\quad \boldsymbol{\mu} =
\begin{pmatrix}
\boldsymbol{\mu}^0 \\ \boldsymbol{\mu}^\mathcal{A} 
\end{pmatrix}, \quad 
\Sigma =
\begin{pmatrix}
\Sigma^0 & \mathbf{0} \\
\mathbf{0} & \Sigma^\mathcal{A}
\end{pmatrix},
\quad n = n_0 + n_\mathcal{A}
\]


The active set selected by applying the Lasso to \(\mathbf{y}^0_{obs}\):
\[\mathcal{M}_{obs} = \{j: \hat{\beta}_j \neq 0 \}\]
\textbf{Statistical inference for the selected feature}\\

\textbf{Hypothesis:}
% \[
% \mathrm{H}_{0,j} : \boldsymbol{\eta}_j^\top \boldsymbol{\mu} = 0 \quad \text{vs.} \quad \mathrm{H}_{1,j} : \boldsymbol{\eta}_j^\top \boldsymbol{\mu} \neq 0.
% \]
\[
\mathrm{H}_{0,j} : \beta_j = 0 \quad \text{vs.} \quad \mathrm{H}_{1,j} : \beta_j \neq 0, \quad \forall j \in \mathcal{M}_{obs}
\]

\textbf{Test statistic:}
\[\hat{\beta_j} = \left(\mathbf{X}^0_{\mathcal{M}_{obs}}(\mathbf{X}^0_{\mathcal{M}_{obs}}^\top \mathbf{X}^0_{\mathcal{M}_{obs}})^{-1}\mathbf{e}_j\right)^\top \mathbf{y}^0_{obs} = \boldsymbol{\eta}^\top_j\mathbf{y}_{obs} \]
where: 
\[\boldsymbol{\eta}_j = \mathcal{X}\mathbf{e}_j,\; 
\mathcal{X} = \begin{pmatrix}
\mathbf{X}^0_{\mathcal{M}_{obs}}(\mathbf{X}^0_{\mathcal{M}_{obs}}^\top \mathbf{X}^0_{\mathcal{M}_{obs}})^{-1} \\ \mathbf{0}_{n_{\mathcal{A}}\times|\mathcal{M}_{obs}|}
\end{pmatrix} , \;\mathbf{e}_j \in \mathbb{R}^{|\mathcal{M}_{obs}|}
\]

The active set selected by applying the Lasso to \(\mathbf{y}^\mathcal{A}_{obs}\):
\[\mathcal{W}_{obs} = \mathcal{W}(\mathbf{y}_{obs})= \{j: \hat{w}_j \neq 0 \}\]

The active set selected by applying the Lasso to \((\mathbf{y}^0_{obs}-\mathbf{X}^0\hat{\boldsymbol{w}})\):
\[\Delta_{obs} = \Delta(\mathbf{y}_{obs})= \{j: \hat{\delta}_j \neq 0 \}\]

\textcolor{red}{\textbf{Selection event:}}
\[
\boldsymbol{\eta}_j^\top \mathbf{y} \mid \left\{ \mathcal{W}(\mathbf{y}) = \mathcal{W}(\mathbf{y}_{obs}), \,\Delta(\mathbf{y}) = \Delta(\mathbf{y}_{obs}), \, \mathbf{q}(\mathbf{y}) = \mathbf{q}(\mathbf{y}_{obs}) \right\}, \tag{1}
\]
\[
\text{where } \mathbf{q}(\mathbf{y}) = \left( \mathbf{I}_n - \mathbf{c} \boldsymbol{\eta}_j^\top \right) \mathbf{y} \text{ with } \mathbf{c} = \Sigma \boldsymbol{\eta}_j \left( \boldsymbol{\eta}_j^\top \Sigma \boldsymbol{\eta}_j \right)^{-1}.
\]

Let us define the set of \( \mathbf{y} \in \mathbb{R}^n \) which satisfies the conditions in Equation (1) as
\[
\mathcal{Y} = \{ \mathbf{y} \in \mathbb{R}^n \mid \left\{ \mathcal{W}(\mathbf{y}) = \mathcal{W}(\mathbf{y}_{obs}), \,\Delta(\mathbf{y}) = \Delta(\mathbf{y}_{obs}), \, \mathbf{q}(\mathbf{y}) = \mathbf{q}(\mathbf{y}_{obs}) \right\}. \tag{2}
\]
% According to the second condition, the data in \( \mathcal{Y} \) is restricted to a line (see Sec 6 in Liu et al. (2018), and Fithian et al. (2014)). Therefore, the set \( \mathcal{Y} \) can be re-written, using a scalar parameter \( z \in \mathbb{R} \), as

$\mathcal Y$ can be re-written, using a scalar parameter $z \in \mathbb R$, as
\[
\mathcal{Y} = \{ \mathbf{y}(z) = \mathbf{a} + \mathbf{b}z \mid z \in \mathcal{Z} \}, \tag{3}
\]
where \( \mathbf{a} = \mathbf{q}(\mathbf{y}_{obs}) \), \( \mathbf{b} = \Sigma \boldsymbol{\eta}_j \left( \boldsymbol{\eta}_j^\top \Sigma \boldsymbol{\eta}_j \right)^{-1} \), and
\[
\mathcal{Z} = \{ z \in \mathbb{R} \mid \mathcal{W}(\mathbf{y}) = \mathcal{W}(\mathbf{y}_{obs}), \,\Delta(\mathbf{y}) = \Delta(\mathbf{y}_{obs}) \}. \tag{4}
\]
We have:
\[\mathbf{y}^0(z) = \mathbf{P}\mathbf{y}(z), \; \mathbf{y}^\mathcal{A}(z) = \mathbf{Q}\mathbf{y}(z)\]
where 
\[\mathbf{P} = (\mathbf{I}_{n_0}, \mathbf{0}) \in \mathbb{R}^{n_0 \times n},\; \mathbf{Q} = (\mathbf{0}, \mathbf{I}_{n_\mathcal{A}}) \in \mathbb{R}^{n_\mathcal{A} \times n} \]\\
\textbf{Characterization of truncation region \(\mathcal{Z}\):}
\[
\hat{\boldsymbol{w}}(z) = \arg \min_{\boldsymbol{w} \in \mathbb{R}^p} \left\{ \frac{1}{2}\| \mathbf{y}^{\mathcal{A}}(z) - \mathbf{X}^{\mathcal{A}} \boldsymbol{w} \|_2^2 + \lambda_{\boldsymbol{w}} \| \boldsymbol{w} \|_1 \right\}
\]
\textcolor{red}{* Optimality condition for \(\hat{\boldsymbol{w}}\):}
\[
(\mathbf{X}^\mathcal{A})^\top(\mathbf{X}^\mathcal{A} \hat{\boldsymbol{w}}(z) - \mathbf{y}^\mathcal{A}(z)) + \lambda_{\boldsymbol{w}}\boldsymbol{s}(z) = 0
\]
or
\[
(\mathbf{X}^\mathcal{A})^\top(\mathbf{X}^\mathcal{A} \hat{\boldsymbol{w}}(z) - \mathbf{Q}\mathbf{y}(z)) + \lambda_{\boldsymbol{w}}\boldsymbol{s}(z) = 0
\]\\
Consider two real values \( z' \) and \( z \) (\( z' > z \)). 
Suppose \( |s_j(z)| < 1 \) for all \( j \in \mathcal{W}_z^c \), \( |s_j(z')| < 1 \) 
for all \( j \in \mathcal{W}_{z'}^c \), and \( (\mathbf{X}_{\mathcal{W}_z}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_z}^\mathcal{A} \) 
is invertible. If \( \hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) \) and \( \hat{\boldsymbol{w}}_{\mathcal{W}_{z'}}(z') \) 
have the \textbf{same active set and the same signs}, then we have:
\[
(\mathbf{X}_{\mathcal{W}_z}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_z}^\mathcal{A}\hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) - (\mathbf{X}_{\mathcal{W}_z}^\mathcal{A})^\top\mathbf{Q}\mathbf{y}(z) + \lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_z}(z) = 0, \tag{5}
\]
\[
(\mathbf{X}_{\mathcal{W}_{z'}}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z'}}^\mathcal{A}\hat{\boldsymbol{w}}_{\mathcal{W}_{z'}}(z') - (\mathbf{X}_{\mathcal{W}_{z'}}^\mathcal{A})^\top\mathbf{Q}\mathbf{y}(z') + \lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z'}}(z') = 0 \tag{6}
\]
(6) - (5):
\[\hat{\boldsymbol{w}}_{\mathcal{W}_{z'}}(z')  - 
 \hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) = ((\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^{-1}(\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{Q}(\mathbf{y}(z') - \mathbf{y}(z))\]
\[
=((\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^{-1}(\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{Q}(\mathbf{a} + \mathbf{b}z' - \mathbf{a} - \mathbf{b}z)
\]
\[
=((\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^{-1}(\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{Q}\mathbf{b} (z' - z)
\]\\
Set \(\boldsymbol{\psi}_{\mathcal{W}_{z}}(z) = ((\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^{-1}(\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A})^\top\mathbf{Q}\mathbf{b} \), we have:
\[{\hat{\boldsymbol{w}}}_{\mathcal{W}_{z'}}(z')  - 
\hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) = \boldsymbol{\psi}_{\mathcal{W}_{z}}(z) \times (z' - z)
\tag{7}\]\\
From the optimality conditions of the Lasso, we also have:
\[
- (\mathbf{X}_{\mathcal{W}_z^c}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_z}^\mathcal{A}\hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) + (\mathbf{X}_{\mathcal{W}_z^c}^\mathcal{A})^\top\mathbf{Q}\mathbf{y}(z) = \lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_z^c}(z), \tag{8}
\]
\[
- (\mathbf{X}_{\mathcal{W}_{z'}^c}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z'}}^\mathcal{A}\hat{\boldsymbol{w}}_{\mathcal{W}_{z'}}(z') + (\mathbf{X}_{\mathcal{W}_{z'}^c}^\mathcal{A})^\top\mathbf{Q}\mathbf{y}(z') = \lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z'}^c}(z') \tag{9}
\]\\
(9) - (8):
\[
\lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z'}^c}(z')-\lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z}^c}(z) = (\mathbf{X}_{\mathcal{W}_{z}^c}^\mathcal{A})^\top\mathbf{Q}\mathbf{b}(z'-z) - (\mathbf{X}_{\mathcal{W}_{z}^c}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A}\boldsymbol{\psi}_{\mathcal{W}_{z}}(z)(z'-z)
\]
\[= \left ((\mathbf{X}_{\mathcal{W}_{z}^c}^\mathcal{A})^\top\mathbf{Q}\mathbf{b} -(\mathbf{X}_{\mathcal{W}_{z}^c}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A}\boldsymbol{\psi}_{\mathcal{W}_{z}}(z) \right)(z'-z) 
\]\\
Set \(\boldsymbol{\gamma}_{\mathcal{W}_{z}^c}(z) =(\mathbf{X}_{\mathcal{W}_{z}^c}^\mathcal{A})^\top\mathbf{Q}\mathbf{b} -(\mathbf{X}_{\mathcal{W}_{z}^c}^\mathcal{A})^\top\mathbf{X}_{\mathcal{W}_{z}}^\mathcal{A}\boldsymbol{\psi}_{\mathcal{W}_{z}}(z) \), we have:
\[
\lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z'}^c}(z')-\lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z}^c}(z) = \boldsymbol{\gamma}_{\mathcal{W}_{z}^c}(z) \times (z'-z) \tag{10}
\]\\
\rule{\linewidth}{0.5mm}
\[
\hat{\boldsymbol{\delta}}(z) = \arg \min_{\boldsymbol{\delta} \in \mathbb{R}^p} \left\{ \frac{1}{2} \| (\mathbf{y}^0(z) - \mathbf{X}^0 \hat{\boldsymbol{w}}(z)}) - \mathbf{X}^0\boldsymbol{\delta} -  \|_2^2 + \lambda_{\boldsymbol{\delta}} \| \boldsymbol{\delta} \|_1 \right\} 
\]
\textcolor{red}{* Optimality condition for \(\hat{\boldsymbol{\delta}}\):}
\[
(\mathbf{X}^0)^\top \left(\mathbf{X}^0\hat{\boldsymbol{\delta}}(z) - \left(\mathbf{y}^0(z) - \mathbf{X}^0 \hat{\boldsymbol{w}}(z)}\right)\right) +  \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}(z) = 0
\]
or
\[
(\mathbf{X}^0)^\top \left(\mathbf{X}^0\hat{\boldsymbol{\delta}}(z) - \left(\mathbf{P}\mathbf{y}(z) - \mathbf{X}_{\mathcal{W}_z}^0 \hat{\boldsymbol{w}}_{\mathcal{W}}_z(z)\right)\right) +  \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}(z) = 0
\]\\
Consider two real values \( z' \) and \( z \) (\( z' > z \)). 
Suppose \( |s'_j(z)| < 1 \) for all \( j \in \Delta_z^c \), \( |s'_j(z')| < 1 \) 
for all \( j \in \Delta_{z'}^c \), and \( (\mathbf{X}_{\Delta_{z}}^0)^\top\mathbf{X}_{\Delta_{z}}^0 \) 
is invertible. If \( \hat{\boldsymbol{\delta}}_{\Delta_z}(z) \) and \( \hat{\boldsymbol{\delta}}_{\Delta_z}(z') \) 
have the \textbf{same active set and the same signs}, then we have:
\[
(\mathbf{X}_{\Delta_z}^0)^\top\mathbf{X}_{\Delta_z}^0\hat{\boldsymbol{\delta}}_{\Delta_z}(z) -(\mathbf{X}_{\Delta_z}^0)^\top \left( \mathbf{P}\mathbf{y}(z) - \mathbf{X}_{\mathcal{W}_z}^0 \hat{\boldsymbol{w}}_{\mathcal{W}}_z(z) \right) +  \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_z}(z) = 0, \tag{11}
\]
\[
(\mathbf{X}_{\Delta_{z'}}^0)^\top\mathbf{X}_{\Delta_{z'}}^0\hat{\boldsymbol{\delta}}_{\Delta_{z'}}(z') -(\mathbf{X}_{\Delta_{z'}}^0)^\top \left(\mathbf{P}\mathbf{y}(z') - \mathbf{X}_{\mathcal{W}_{z'}}^0 \hat{\boldsymbol{w}}_{\mathcal{W}}_{z'}(z') \right) +  \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_{z'}}(z') = 0 \tag{12}
\]\\
(12) - (11):
\[
\hat{\boldsymbol{\delta}}_{\Delta_{z}}(z') - \hat{\boldsymbol{\delta}}_{\Delta_{z}}(z) = \left((\mathbf{X}_{\Delta_{z}}^0)^\top\mathbf{X}_{\Delta_{z}}^0 \right)^{-1}(\mathbf{X}_{\Delta_{z}}^0)^\top \left(\mathbf{P} \left(\mathbf{y}(z') - \mathbf{y}(z) \right) - \mathbf{X}^0_{\mathcal{W}_z} \left( \hat{\boldsymbol{w}}_{\mathcal{W}_{z'}}(z')  - 
 \hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) \right)
\right)
\]
\[=
\left((\mathbf{X}_{\Delta_{z}}^0)^\top\mathbf{X}_{\Delta_{z}}^0 \right)^{-1}(\mathbf{X}_{\Delta_{z}}^0)^\top \left(\mathbf{P}\mathbf{b} (z' - z) - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_{z}}(z)(z' - z)
\right)
\]
\[=
\left((\mathbf{X}_{\Delta_{z}}^0)^\top\mathbf{X}_{\Delta_{z}}^0 \right)^{-1}(\mathbf{X}_{\Delta_{z}}^0)^\top \left(\mathbf{P}\mathbf{b} - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_{z}}(z)
\right)(z' - z)
\]\\
Set \(\boldsymbol{\nu}_{\Delta_{z}}(z) = \left((\mathbf{X}_{\Delta_{z}}^0)^\top\mathbf{X}_{\Delta_{z}}^0 \right)^{-1}(\mathbf{X}_{\Delta_{z}}^0)^\top \left(\mathbf{P}\mathbf{b} - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_{z}}(z)
\right)\), we have:
\[
\hat{\boldsymbol{\delta}}_{\Delta_{z}}(z') - \hat{\boldsymbol{\delta}}_{\Delta_{z}}(z) = \boldsymbol{\nu}_{\Delta_{z}}(z) \times (z-z') \tag{13}
\]
From the optimality conditions of the Lasso, we also have:
\[
-(\mathbf{X}_{\Delta_z^c}^0)^\top\mathbf{X}_{\Delta_z}^0\hat{\boldsymbol{\delta}}_{\Delta_z}(z) + (\mathbf{X}_{\Delta_z^c}^0)^\top \left(\mathbf{P}\mathbf{y}(z) - \mathbf{X}_{\mathcal{W}_z}^0 \hat{\boldsymbol{w}}_{\mathcal{W}}_z(z) \right) =  \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_z^c}(z), \tag{14}
\]
\[
-(\mathbf{X}_{\Delta_{z'}^c}^0)^\top\mathbf{X}_{\Delta_{z'}}^0\hat{\boldsymbol{\delta}}_{\Delta_{z'}}(z') +(\mathbf{X}_{\Delta_{z'}^c}^0)^\top \left(\mathbf{P}\mathbf{y}(z') - \mathbf{X}_{\mathcal{W}_{z'}}^0 \hat{\boldsymbol{w}}_{\mathcal{W}}_{z'}(z') \right) =  \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_{z'}^c}(z') \tag{15}
\]\\
(15) - (14):
\[
\lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_{z'}^c}(z') - \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_z^c}(z) = (\mathbf{X}_{\Delta_{z}^c}^0)^\top\left(\mathbf{P}\mathbf{b} - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_{z}}(z)
\right)(z' - z) - (\mathbf{X}_{\Delta_z^c}^0)^\top\mathbf{X}_{\Delta_z}^0\boldsymbol{\nu}_{\Delta_{z}}(z)(z-z')
\]
\[
=
\left((\mathbf{X}_{\Delta_{z}^c}^0)^\top\left(\mathbf{P}\mathbf{b} - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_{z}}(z)
\right) - (\mathbf{X}_{\Delta_z^c}^0)^\top\mathbf{X}_{\Delta_z}^0\boldsymbol{\nu}_{\Delta_{z}}(z)\right)(z-z')
\]\\
Set \(
\boldsymbol{\kappa}_{\Delta_z^c} = (\mathbf{X}_{\Delta_{z}^c}^0)^\top\left(\mathbf{P}\mathbf{b} - \mathbf{X}^0_{\mathcal{W}_z} \boldsymbol{\psi}_{\mathcal{W}_{z}}(z)
\right) - (\mathbf{X}_{\Delta_z^c}^0)^\top\mathbf{X}_{\Delta_z}^0\boldsymbol{\nu}_{\Delta_{z}}(z)
\), we have: 
\[
\lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_{z'}^c}(z') - \lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_z^c}(z) = \boldsymbol{\kappa}_{\Delta_z^c}(z'-z) \tag{16}
\]
\rule{\linewidth}{0.5mm} % Kẻ một đường ngang toàn trang
\textbf{Lemma.} Let \( z \) be a real value such that 
\(
\max_{j \in \mathcal{W}_z^c} \lvert s_j(z) \rvert < 1
\), and \(
\max_{j \in {\Delta}_z^c} \lvert s'_j(z) \rvert < 1.
\)
Then, \(\mathcal{W}_{z'} = \mathcal{W}_z\), \(\Delta_{z'} = \Delta_z\)
\(
\max_{j \in \mathcal{W}_{z'}^c} \lvert s_j(z') \rvert < 1,
\)
\(
\max_{j \in \Delta_{z'}^c} \lvert s'_j(z') \rvert < 1,
\)
\(\boldsymbol{s}(z) = \boldsymbol{s}(z'),\) and \(\boldsymbol{s'}(z) = \boldsymbol{s'}(z')\) for any real value \(z'\) in the interval \([z, z + t_z]\), where \(z + t_z\) is the value of the transition point,
\[
t_z = \min \left\{ t_z^1, t_z^2, t_z^3, t_z^4 \right\},
\]
\[
t_z^1 = \min_{j \in \mathcal{W}_z} \left( -\frac{\hat{w}_j(z)}{\psi_j(z)} \right)_{++},
\]
\[
t_z^2 = \min_{j \in \mathcal{W}_z^c} \left( \lambda_{\boldsymbol{w}}\frac{\text{sign}(\gamma_j(z)) - s_j(z)}{\gamma_j(z)} \right)_{++},
\]
\[
t_z^3 = \min_{j \in \Delta_z} \left( -\frac{\hat{\delta}_j(z)}{\nu_j(z)} \right)_{++},
\]
\[
t_z^4 = \min_{j \in \Delta_z^c} \left(\lambda_{\boldsymbol{\delta}} \frac{\text{sign}(\kappa_j(z)) - s'_j(z)}{\kappa_j(z)} \right)_{++}.
\]
Here, we use the convention that for any \(m \in \mathbb{R}\),
\[
(m)_{++} = 
\begin{cases} 
m, & \text{if } m > 0, \\ 
\infty, & \text{otherwise}.
\end{cases}
\]\\
\textbf{Proof}\\
From Equation (7), we have:
\[{\hat{\boldsymbol{w}}}_{\mathcal{W}_{z'}}(z')  - 
\hat{\boldsymbol{w}}_{\mathcal{W}_z}(z) = \boldsymbol{\psi}_{\mathcal{W}_{z}}(z) \times (z' - z)\]

To guarantee ${\hat{\boldsymbol{w}}}_{\mathcal{W}_{z'}}(z')$ and $\hat{\boldsymbol{w}}_{\mathcal{W}_z}(z)$ have the same signs, 
\[
s_j(z') = s_j(z), \quad \forall j \in \mathcal{W}_z. \tag{17}
\]

For a specific $j \in \mathcal{W}_z$, we consider the following cases:

\begin{itemize}
    \item If $\hat{w}_j(z) > 0$, then $\hat{w}_j(z') = \hat{w}_j(z) + \psi_j(z) \times (z' - z) > 0$.
    \begin{itemize}
        \item If $\psi_j(z) > 0$, then $z' - z > -\frac{\hat{w}_j(z)}{\psi_j(z)}$ (This inequality always holds since the left-hand side is positive while the right-hand side is negative).
        \item If $\psi_j(z) < 0$, then $z' - z < -\frac{\hat{w}_j(z)}{\psi_j(z)}$.
    \end{itemize}
    \item If $\hat{w}_j(z) < 0$, then $\hat{w}_j(z') = \hat{w}_j(z) + \psi_j(z) \times (z' - z) < 0$.
    \begin{itemize}
        \item If $\psi_j(z) > 0$, then $z' - z < -\frac{\hat{w}_j(z)}{\psi_j(z)}$.
        \item If $\psi_j(z) < 0$, then $z' - z > -\frac{\hat{w}_j(z)}{\psi_j(z)}$ (This inequality always holds since the left-hand side is positive while the right-hand side is negative).
    \end{itemize}
\end{itemize}

Finally, for satisfying the condition in Equation~(17),
\[
z' - z \leq \min_{j \in \mathcal{W}_z} \left( -\frac{\hat{w}_j(z)}{\psi_j(z)} \right)_{++} = t^{1}_z.
\]\\
From Equation~(10), we have:
\[
\lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z'}^c}(z')-\lambda_{\boldsymbol{w}}\boldsymbol{s}_{\mathcal{W}_{z}^c}(z) = \boldsymbol{\gamma}_{\mathcal{W}_{z}^c}(z) \times (z'-z).
\]

To guarantee $\|\lambda_{\boldsymbol{w}} s_{\mathcal{W}_z^c}(z')\|_\infty = \|\lambda_{\boldsymbol{w}} s_{\mathcal{W}_z^c}(z) + \gamma_{\mathcal{W}_z^c}(z) \times (z' - z)\|_\infty < \lambda_{\boldsymbol{w}}$,
\[
-\lambda_{\boldsymbol{w}} < \lambda_{\boldsymbol{w}} s_j(z) + \gamma_j(z) \times (z' - z) < \lambda_{\boldsymbol{w}}, \quad \forall j \in \mathcal{W}_z^c. \tag{18}
\]

For a specific $j \in \mathcal{W}_z^c$, we have the following cases:

\begin{itemize}
    \item If $\gamma_j(z) > 0$, then 
    \[
    \frac{-\lambda_{\boldsymbol{w}} - \lambda_{\boldsymbol{w}} s_j(z)}{\gamma_j(z)} < z' - z < \frac{\lambda_{\boldsymbol{w}} - \lambda_{\boldsymbol{w}} s_j(z)}{\gamma_j(z)}.
    \]
    \item If $\gamma_j(z) < 0$, then 
    \[
    \frac{\lambda_{\boldsymbol{w}} - \lambda_{\boldsymbol{w}} s_j(z)}{\gamma_j(z)} < z' - z < \frac{-\lambda_{\boldsymbol{w}} - \lambda_{\boldsymbol{w}} s_j(z)}{\gamma_j(z)}.
    \]
\end{itemize}

Note that the first inequalities of the above two cases always hold since the left-hand side is negative while the right-hand side is positive. Then, for satisfying the condition in Equation~(18),
\[
z' - z < \min_{j \in \mathcal{W}_z^c} \left( \lambda_{\boldsymbol{w}}\frac{\text{sign}(\gamma_j(z)) - s_j(z)}{\gamma_j(z)} \right)_{++} = t_z^2.
\]\\
From Equation (13), we have:
\[\hat{\boldsymbol{\delta}}_{\Delta_{z}}(z') - \hat{\boldsymbol{\delta}}_{\Delta_{z}}(z) = \boldsymbol{\nu}_{\Delta_{z}}(z) \times (z-z')\]

To guarantee ${\hat{\boldsymbol{\delta}}}_{\Delta_{z'}}(z')$ and $\hat{\boldsymbol{\delta}}_{\Delta_z}(z)$ have the same signs, 
\[
s'_j(z') = s'_j(z), \quad \forall j \in \Delta_z. \tag{19}
\]

For a specific $j \in \Delta_z$, we consider the following cases:

\begin{itemize}
    \item If $\hat{\delta}_j(z) > 0$, then $\hat{\delta}_j(z') = \hat{\delta}_j(z) + \nu_j(z) \times (z' - z) > 0$.
    \begin{itemize}
        \item If $\nu_j(z) > 0$, then $z' - z > -\frac{\hat{\delta}_j(z)}{\nu_j(z)}$ (This inequality always holds since the left-hand side is positive while the right-hand side is negative).
        \item If $\nu_j(z) < 0$, then $z' - z < -\frac{\hat{\delta}_j(z)}{\nu_j(z)}$.
    \end{itemize}
    \item If $\hat{\delta}_j(z) < 0$, then $\hat{\delta}_j(z') = \hat{\delta}_j(z) + \nu_j(z) \times (z' - z) < 0$.
    \begin{itemize}
        \item If $\nu_j(z) > 0$, then $z' - z < -\frac{\hat{\delta}_j(z)}{\nu_j(z)}$.
        \item If $\nu_j(z) < 0$, then $z' - z > -\frac{\hat{\delta}_j(z)}{\nu_j(z)}$ (This inequality always holds since the left-hand side is positive while the right-hand side is negative).
    \end{itemize}
\end{itemize}

Finally, for satisfying the condition in Equation~(19),
\[
z' - z \leq \min_{j \in \Delta_z} \left( -\frac{\hat{\delta}_j(z)}{\nu_j(z)} \right)_{++} = t^{3}_z.
\]
From Equation~(16), we have:
\[
\lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_{z'}^c}(z')-\lambda_{\boldsymbol{\delta}}\boldsymbol{s'}_{\Delta_{z}^c}(z) = \boldsymbol{\kappa}_{\Delta_{z}^c}(z) \times (z'-z).
\]

To guarantee $\|\lambda_{\boldsymbol{\delta}} s'_{\Delta_z^c}(z')\|_\infty = \|\lambda_{\boldsymbol{\delta}} s_{\Delta_z^c}(z) + \kappa_{\Delta_z^c}(z) \times (z' - z)\|_\infty < \lambda_{\boldsymbol{\delta}}$,
\[
-\lambda_{\boldsymbol{\delta}} < \lambda_{\boldsymbol{\delta}} s_j(z) + \kappa_j(z) \times (z' - z) < \lambda_{\boldsymbol{\delta}}, \quad \forall j \in \Delta_z^c. \tag{20}
\]

For a specific $j \in \Delta_z^c$, we have the following cases:

\begin{itemize}
    \item If $\kappa_j(z) > 0$, then 
    \[
    \frac{-\lambda_{\boldsymbol{\delta}} - \lambda_{\boldsymbol{\delta}} s'_j(z)}{\kappa_j(z)} < z' - z < \frac{\lambda_{\boldsymbol{\delta}} - \lambda_{\boldsymbol{\delta}} s'_j(z)}{\kappa_j(z)}.
    \]
    \item If $\kappa_j(z) < 0$, then 
    \[
    \frac{\lambda_{\boldsymbol{\delta}} - \lambda_{\boldsymbol{\delta}} s'_j(z)}{\kappa _j(z)} < z' - z < \frac{-\lambda_{\boldsymbol{\delta}} - \lambda_{\boldsymbol{\delta}} s'_j(z)}{\kappa_j(z)}.
    \]
\end{itemize}

Note that the first inequalities of the above two cases always hold since the left-hand side is negative while the right-hand side is positive. Then, for satisfying the condition in Equation~(20),
\[
z' - z < \min_{j \in \Delta_z^c} \left( \lambda_{\boldsymbol{\delta}}\frac{\text{sign}(\kappa_j(z)) - s'_j(z)}{\kappa_j(z)} \right)_{++} = t_z^4.
\]
Finally, we can compute $t_z$ by taking 
\[
t_z = \min \left\{ t_z^1, t_z^2, t_z^3, t_z^4 \right\}.
\]
\end{document}



3. ĐÂY LÀ 3 FILES CODE:
p_value.py:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso

import gen_data  # Ensure gen_data.py is correctly implemented
import parametric_lasso
import util

def run_simulation(num_simulations=500):
    p = 10
    s = 0  # Number of non-zero coefficients in true beta
    n0 = 150  # Size of primary dataset
    num_aux_dataset = 8
    n_aux_single = 100  # Size of each auxiliary dataset
    nA = num_aux_dataset * n_aux_single  # Total auxiliary data size

    lamda_w = 0.05
    lamda_delta = 0.08

    sig_beta = 0.3
    gamma = 0.2
    threshold = 20
    p_values = []
    n = n0 + nA  # Total data size

    for sim in range(num_simulations):
        if (sim + 1) % 50 == 0 or sim == 0:
            print("----------------Run", sim + 1, "----------------")

        # Generate data
        X_stack, y_stack, n_vec, _ = gen_data.generate_data(p=p, n0=n0, num_aux_dataset=num_aux_dataset, s=s, sig_beta=sig_beta, gamma=gamma)
        # Reshape y_stack
        y_stack = y_stack.reshape(-1, 1)

        # Run Oracle Trans Lasso to get beta_hat
        beta_hat, w_hat, delta_hat = parametric_lasso.OracleTransLasso(
            X_stack, y_stack, n0, nA, lamda_w, lamda_delta
        )

        if (sim+1)%50==0 or sim==0:
            # print (f'beta_hat: {beta_hat}')
            # print (f'w_hat: {w_hat}')
            # print (f'delta_hat: {delta_hat}')
            
            print(f'Number of non-zero elements in beta_hat: {np.count_nonzero(beta_hat)}')
            print(f'Number of non-zero elements in w_hat: {np.count_nonzero(w_hat)}')
            print(f'Number of non-zero elements in delta_hat: {np.count_nonzero(delta_hat)}')


        X_0 = X_stack[:n0]
        y_0 = y_stack[:n0]

        # Construct M and X0M
        M, X0M = util.construct_M_X0M(X_0, beta_hat, p)

        if len(M) == 0:
            continue  # Skip if no active features selected

        for j_selected in M:
            # Construct test statistic
            etaj, etajTy = util.test_statistic(j_selected, X0M, y_stack, M, n0, n)

            # Run parametric oracle trans lasso
            list_zk, list_beta_hat = parametric_lasso.run_parametric_oracle_trans_lasso(
                X_stack, y_stack, n0, nA, lamda_w, lamda_delta, etaj, threshold, p
            )

            # Compute p-value
            p_value = util.p_value(M, beta_hat, list_zk, list_beta_hat, etaj, etajTy)

            if p_value is not None:
                p_values.append(p_value)

    # Plot histogram of p-values
    if len(p_values) > 0:
        plt.hist(p_values, bins=20, edgecolor='k', density=True)
        plt.xlabel('p-value')
        plt.ylabel('Density')
        plt.title('Histogram of p-values under the Null Hypothesis')
        plt.savefig('pvalue_histogram.png')
        plt.show()
    else:
        print("No p-values were collected.")

    print(f"Number of p-values collected: {len(p_values)}")

if __name__ == '__main__':
    run_simulation()


util.py:
import numpy as np
from mpmath import mp

mp.dps = 500  # Set decimal precision for mpmath

def test_statistic(j, X0M, y, M, n0, n):
    ej = np.zeros((len(M), 1))
    idx = M.index(j)
    ej[idx] = 1

    inv = np.linalg.pinv(np.dot(X0M.T, X0M))
    X0M_inv = np.dot(X0M, inv)

    # Create extended matrix _X to match the dimensions of y
    _X = np.zeros((n, len(M)))
    _X[:n0, :] = X0M_inv
    etaj = np.dot(_X, ej)

    etajTy = np.dot(etaj.T, y)[0][0]

    return etaj, etajTy

def construct_M_X0M(X0, beta_hat, p, epsilon=1e-10):
    M = []
    for i in range(p):
        if abs(beta_hat[i]) > epsilon:
            M.append(i)

    X0M = X0[:, M]

    return M, X0M

def construct_wW_W_XAW_Wc_XAWc(XA, w_hat, p, epsilon=1e-10):
    W = []
    wW = []
    Wc = []

    for i in range(p):
        if abs(w_hat[i]) > epsilon:
            W.append(i)
            wW.append(w_hat[i])
        else:
            Wc.append(i)

    XAW = XA[:, W]
    XAWc = XA[:, Wc]

    wW = np.array(wW).reshape(-1, 1)

    return wW, W, XAW, Wc, XAWc

def construct_dD_D_X0D_Dc_X0Dc(X0, delta_hat, p, epsilon=1e-10):
    D = []
    dD = []
    Dc = []

    for i in range(p):
        if abs(delta_hat[i]) > epsilon:
            D.append(i)
            dD.append(delta_hat[i])
        else:
            Dc.append(i)

    X0D = X0[:, D]
    X0Dc = X0[:, Dc]

    dD = np.array(dD).reshape(-1, 1)

    return dD, D, X0D, Dc, X0Dc

def construct_P_Q(n0, nA):
    n = n0 + nA
    P = np.zeros((n0, n))
    Q = np.zeros((nA, n))

    P[:, :n0] = np.identity(n0)
    Q[:, n0:] = np.identity(nA)

    return P, Q

def p_value(M, beta_hat, list_zk, list_beta_hat, etaj, etajTy):
    # Assuming that etajTy follows a truncated normal distribution
    # We need to compute the truncation intervals based on list_zk
    # and the selection event

    tn_mu = 0
    tn_sigma = np.sqrt(np.dot(etaj.T, etaj)[0][0])

    z_intervals = []
    for i in range(len(list_zk) - 1):
        z_intervals.append([list_zk[i], list_zk[i + 1]])

    numerator = 0
    denominator = 0

    for interval in z_intervals:
        al, ar = interval
        denominator += mp.ncdf((ar - tn_mu) / tn_sigma) - mp.ncdf((al - tn_mu) / tn_sigma)
        if etajTy >= ar:
            numerator += mp.ncdf((ar - tn_mu) / tn_sigma) - mp.ncdf((al - tn_mu) / tn_sigma)
        elif (etajTy >= al) and (etajTy < ar):
            numerator += mp.ncdf((etajTy - tn_mu) / tn_sigma) - mp.ncdf((al - tn_mu) / tn_sigma)

    if denominator != 0:
        value = float(numerator / denominator)
        return 2 * min(1 - value, value)
    else:
        return None



parametric_lasso.py:
import numpy as np
from sklearn.linear_model import Lasso
import util

def calculate_quotient(numerator, denominator):
    if denominator == 0:
        return np.inf
    quotient = numerator / denominator
    if quotient <= 0:
        return np.inf
    return quotient

def OracleTransLasso(X, y, n0, nA, lamda_w, lamda_delta):
    X0 = X[:n0]
    y0 = y[:n0]
    XA = X[n0:]
    yA = y[n0:]

    # Fit w_hat
    clf_w = Lasso(alpha=lamda_w, fit_intercept=False, tol=1e-10)
    clf_w.fit(XA, yA.flatten())
    w_hat = clf_w.coef_

    # Compute residual
    residual = y0.flatten() - X0 @ w_hat

    # Fit delta_hat
    clf_delta = Lasso(alpha=lamda_delta, fit_intercept=False, tol=1e-10)
    clf_delta.fit(X0, residual)
    delta_hat = clf_delta.coef_

    beta_hat = w_hat + delta_hat

    return beta_hat, w_hat, delta_hat

def compute_step_size(X, yz, P, Q, lamda_w, lamda_delta, b, n0, nA, p):
    beta_hat, w_hat, delta_hat = OracleTransLasso(X, yz, n0, nA, lamda_w, lamda_delta)
    X0 = X[:n0]
    XA = X[n0:]

    wW, W, XAW, Wc, XAWc = util.construct_wW_W_XAW_Wc_XAWc(XA, w_hat, p)
    dD, D, X0D, Dc, X0Dc = util.construct_dD_D_X0D_Dc_X0Dc(X0, delta_hat, p)

    # Calculate psiWz
    if XAW.shape[1] > 0:
        inv = np.linalg.pinv(np.dot(XAW.T, XAW))
        inv_XAWT = np.dot(inv, XAW.T)
        psiWz = inv_XAWT @ (Q @ b)
    else:
        psiWz = np.zeros((0, 1))  # Ensure psiWz has correct shape

    # Calculate gammaWcz and sWcz
    if XAWc.shape[1] > 0:
        if XAW.shape[1] == 0:
            gammaWcz = XAWc.T @ (Q @ b)
            e1 = Q @ yz
        else:
            gammaWcz = XAWc.T @ (Q @ b) - XAWc.T @ XAW @ psiWz
            e1 = Q @ yz - XAW @ wW.reshape(-1, 1)

        e2 = XAWc.T @ e1
        sWcz = e2 / lamda_w
    else:
        gammaWcz = np.zeros((0, 1))
        sWcz = np.zeros((0, 1))

    # Prepare e3
    X0W = X0[:, W]
    if X0W.shape[1] > 0:
        X0W_psiWz = X0W @ psiWz
    else:
        X0W_psiWz = np.zeros((n0, 1))

    e3 = P @ b - X0W_psiWz

    # Calculate nuDz
    if X0D.shape[1] > 0:
        inv_ = np.linalg.pinv(X0D.T @ X0D)
        inv_X0DT = inv_ @ X0D.T
        nuDz = inv_X0DT @ e3
    else:
        nuDz = np.zeros((0, 1))

    # Calculate kappaDcz and sDcz
    if X0Dc.shape[1] > 0:
        if X0D.shape[1] == 0:
            # In this case, e3 is defined, and X0D is empty
            kappaDcz = X0Dc.T @ e3
            e4 = e3
        else:
            kappaDcz = X0Dc.T @ e3 - X0Dc.T @ X0D @ nuDz
            e4 = e3 - X0D @ dD.reshape(-1, 1)

        e5 = X0Dc.T @ e4
        sDcz = e5 / lamda_delta
    else:
        kappaDcz = np.zeros((0, 1))
        sDcz = np.zeros((0, 1))

    # Flatten arrays for further computations
    wW = wW.flatten()
    dD = dD.flatten()
    psiWz = psiWz.flatten()
    gammaWcz = gammaWcz.flatten()
    nuDz = nuDz.flatten()
    kappaDcz = kappaDcz.flatten()
    sWcz = sWcz.flatten()
    sDcz = sDcz.flatten()

    t1 = np.inf
    t2 = np.inf
    t3 = np.inf
    t4 = np.inf

    # Calculate t1
    for j in range(len(W)):
        numerator = -wW[j]
        denominator = psiWz[j]
        quotient = calculate_quotient(numerator, denominator)
        if quotient < t1:
            t1 = quotient

    # Calculate t2
    for j in range(len(Wc)):
        numerator = lamda_w * (np.sign(gammaWcz[j]) - sWcz[j])
        denominator = gammaWcz[j]
        quotient = calculate_quotient(numerator, denominator)
        if quotient < t2:
            t2 = quotient

    # Calculate t3
    for j in range(len(D)):
        numerator = -dD[j]
        denominator = nuDz[j]
        quotient = calculate_quotient(numerator, denominator)
        if quotient < t3:
            t3 = quotient

    # Calculate t4
    for j in range(len(Dc)):
        numerator = lamda_delta * (np.sign(kappaDcz[j]) - sDcz[j])
        denominator = kappaDcz[j]
        quotient = calculate_quotient(numerator, denominator)
        if quotient < t4:
            t4 = quotient

    return min(t1, t2, t3, t4), w_hat, W, delta_hat, D

def run_parametric_oracle_trans_lasso(X, y, n0, nA, lamda_w, lamda_delta, etaj, threshold, p):
    z = -threshold
    n = X.shape[0]
    P, Q = util.construct_P_Q(n0, nA)

    list_zk = [z]
    list_beta_hat = []

    while z < threshold:
        # Compute yz
        sq_norm = np.dot(etaj.T, etaj)[0][0]
        projection = (etaj @ etaj.T) / sq_norm
        e1 = y - projection @ y
        yz = e1 + (etaj / sq_norm) * z

        # Compute b
        b = etaj / sq_norm

        # Compute step size
        t_z, w_hat, W, delta_hat, D = compute_step_size(X, yz, P, Q, lamda_w, lamda_delta, b, n0, nA, p)

        # Update z
        z_new = z + t_z + 0.0001
        if z_new > threshold:
            z_new = threshold
            t_z = z_new - z

        z = z_new
        list_zk.append(z)

        # Store beta_hat
        beta_hat = w_hat + delta_hat
        list_beta_hat.append(beta_hat)

        if z == threshold or t_z == np.inf:
            break

    return list_zk, list_beta_hat
 
code full correct files for SI OracleTransLassso
